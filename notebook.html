<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Engineering Notebook</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      margin: 0 auto;
      max-width: 800px;
      padding: 2em;
      background: #f9f9f9;
      color: #333;
      line-height: 1.6;
    }
    h1, h2 {
      color: #2c3e50;
    }
    h1 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
    }
    ul {
      padding-left: 1.2em;
    }
    img {
      max-width: 75%;
      margin: 1em 0;
      border: 1px solid #ccc;
      border-radius: 6px;
    }
    .entry {
      margin-bottom: 3em;
      padding-bottom: 2em;
      border-bottom: 1px dashed #ccc;
    }
    .note {
      background: #fff;
      padding: 1em;
      border-left: 4px solid #3498db;
      margin: 1em 0;
    }
    .quote {
      background: #eee;
      padding: 0.8em 1em;
      font-style: italic;
      border-left: 4px solid #999;
    }
  </style>
</head>
<body>

  <h1>Engineering Notebook</h1>
  <div class="entry">
  <h2>October 5 - October 6, 2025</h2>
  <ul>
    <li><strong>Progress Check</strong>
      <ul>
        <li><em>Issues files for a GPS location</em>
          <ul>
            <li>This method uses station transmitter class (A, B1, C, etc.) to approximate broadcast range. The FCC uses this to draw the contour maps showing which wavelengths are reserved for that particular station.</li>
            <li>Uses Haversine distance calculations to go from latitude/longitude to a reliable arc length on a sphere (of a known radius).</li>
            <li>The links to retrieve the issues/programs lists are aggregated only if they are within that station's likely protected radius.</li>
            <li>Example output: “Found 39 stations within contour range. Saved results to <code>filtered_stations_medford.json</code>.”</li>
            <li>Note: <strong>raw counts may be useful</strong>.</li>
            <li>1749 individual PDFs processed.</li>
          </ul>
        </li>
        <li><em>ndjson_updated</em>:
          <ul>
            <li>Fields added to label station as commercial or not. Power and height data may prove useful:</li>
            <li>“Effective Radiated Power (horizontal)”</li>
            <li>“Effective Radiated Power (vertical)”</li>
            <li>“Antenna Height Above Average Terrain (HAAT) — horizontal polarization”</li>
            <li>“Antenna Height Above Average Terrain (HAAT) — vertical polarization”</li>
            <li>“Commercial”</li>
          </ul>
        </li>
      </ul>
    </li>

    <li><strong>Todo</strong>
      <ul>
        <li>Optimize process of collecting stations/PDFs:
          <ul>
            <li>GPS mode: basically functional (Oct 5)</li>
            <li>City (Geo/shapefiles/etc.)</li>
            <li>State</li>
            <li>Licensee</li>
          </ul>
        </li>
        <li>Counting:
          <ul>
            <li><strong>File count</strong>: Multiple files may represent the same quarter; consider merging unique submissions into a single logical document.</li>
            <li><strong>Issue count</strong>: Count distinct issues (based on unique dates and program titles).</li>
          </ul>
        </li>
        <li><strong>Similarity of Issues</strong> — exploring clustering and similarity metrics across stations.</li>
      </ul>
    </li>

    <li><strong>Crazy Idea</strong>
      <ul>
        <li>Count the number of stations and files. For any given latitude/longitude, return a series of tuples. The length of the series = number of stations receivable at that location at the time of the FM Query.</li>
        <li><em>Simple numeric description of a region</em>:
          <pre><code>[
  ["commercial", "file_count"],
  ["commercial", "file_count"],
]
[
  [1, 52],
  [0, 12], // Example: 1 commercial and 1 educational station
]</code></pre>
        </li>
      </ul>
    </li>
  </ul>
</div>
  <h2>October 6, 2025 — Proof of Concept Visualization</h2>
  <ul>
    <li><strong>Visualization</strong>:
      <ul>
        <li>The image below shows the <em>proportion of stories in each category</em> as decided by WBUR.</li>
        <li>This serves as a <strong>proof of concept</strong> demonstrating how issue categorization and clustering could be represented visually.</li>
      </ul>
    </li>
  </ul>
  <div class="image-container">
    <img src="images/ME_ATC_test.png" alt="Proportion of stories by category — WBUR proof of concept" style="max-width: 80%; height: auto; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.15);">
    <p class="caption"><em>Figure: WBUR Issue Proportion — Proof of Concept Visualization</em></p>
  </div>
</div>
  <div class="entry">
    <h2>July 7 - July 28, 2025</h2>
    <ul>
      <li>Updating the engineering notebook. July 28</li>
      <li><strong>DESIGN DOCUMENT</strong>: Working on background research. One book, "The Fairness Doctrine" seems to be great for contextualizing the rationale for regulatory reform. Several articles about the FCC push for localism in the 2000's. A CS disertaion investigating FCC enforcement actions. I am also citing work by currently serving politicians for and against relevant regulations</li>
      <li><strong>DESIGN DOCUMENT</strong>: Research Questions are being finalized. (1)  To examine the relationship between `number/location of stations owned/licensed` and the stations' `rate of locally produced content` aired by the station. My hypothesis is that when viewed in aggregate, the large owners will have a lower rate of locally produced content. A simple explanation would be *efficiency*: it is more cost effective to centrally distribute the production costs. (2) Using `NLP and clustering methods` I intend to cluster **issues** by category. (3) The issue-group distributions in (2) are snapshots taken every 3 months. When a station has a record of consistent I+P format and editorial oversight, such as the WBUR example, **issue** prevelance can be monitored over time. This </li>
      <li><strong>DESIGN DOCUMENT</strong>: BibTex for citations is being compiled. I need to ensure all sources are properly cited in the final document.</li>
      <li>Scraping: I have the I+P repos for several dozen stations but this is incomplete. Instead of bulk scraping I want to target specific stations by some grouping. </li>
      <li></li>

    </ul>
  </div>
  <div class="entry">
    <h2>June 11 - July 6, 2025</h2>
    <ul>
      <li>Updating the engineering notebook. July 6</li>

      <div class="entry">
  <h2>TODO</h2>
  <ul>
    <li><strong>Scrape files and repo structure</strong>
      <ul>
        <li><strong>FILES</strong> → ALL accessible I+P</li>
        <li><strong>REPO STRUCTURE</strong> → Can record size, name, relative location, count, size, etc.</li>
      </ul>
    </li>
    <li><strong>TEXT EXTRACTION</strong></li>
    <li>Disambiguation and <strong>labeling text</strong>
      <ul>
        <li>"issues" vs "programs": the text serves different purposes and the first step would be to attempt to separate the sections</li>
        <li>"unique shows" may pose an opportunity to take on the problem of newsroom-human-data-entry. Rough example:
          <ul>
            <li>unique shows → issue → programs[station] → Map of reach for all transmitters</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Topic-identification by show
      <ul>
        <li><strong>Show Specific Labeled Issues</strong> can be tracked as long as the show airs.</li>
        <li>Comparing issues discussed by different newsrooms/stations is tenuous – station-issue-groups are not consistent when comparing the I+P from newsrooms.</li>
        <li>Poorly grouped or labeled issues are likely. Station-based writers may use similar but distinct language to discuss the same show/segment.</li>
      </ul>
    </li>
    <li><strong>FUTURE:</strong>
      <ul>
        <li><strong>MAPPING (possibilities)</strong>
          <ul>
            <li><strong>Story-issue reach</strong></li>
            <li><strong>Station-issue treatment</strong>
              <ul>
                <li>This may involve clustering the stations into subtypes (e.g., commercial-pop vs noncommercial-religious) to generalize the station-types based on issue treatment by region.</li>
              </ul>
            </li>
            <li><strong>Local vs Outsourced Newsroom</strong></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</div>





  <div class="entry">
    <h2>June 03 - June 10, 2025</h2>
    <ul>
      <li>Updating the engineering notebook. June 10</li>
    <div class="note">
      <strong>Summary of Work:</strong><br>
      <ul>
    </ul>

    <div class="note">
      <strong>Summary of Work:</strong><br>
      <ul>
  <li>
    The process of scraping PDFs is tricky so I started the attempt at analyzing the Radio Landscape with a single station. My initial method of scraping the PDFs is heavily reliant on consistent formatting. I am currently working further on this problem
    <ul>
      <li>From the public FCC files WBUR (Boston University's NPR station) I was able to completely capture the tables of issues for 2019Q1-2025Q1.</li>
      <li>
        The full set of I+P reports, 2013-2025, is remarkably consistent. WBUR uses the exact same skeleton for these reports
        <ul>
          <li>WBUR produces two shows "Morning Edition" and "All Things Considered" and that has not changed over the entire 12 year duration of this set.</li>
          <li>The documents are dozens of pages. (70 pages is the last one I skimmed)</li>
          <li>Issues are before the programming lists. Additionally, issues are always ordered as below:
            <ul>
              <li>["General","Economic and Business", "Education", "Medical and Healthcare", "Politics and Government", "Crime", "Science and Technology", "Religion", "Weather and Environmental", "Arts, Culture, Media", "Sports", "National/International"]</li>
              <li>The change in the issue group is noted by the next issue appearing in its own line. This makes it easy to tie the issue groups to the blurbs. The authors of WBUR's I+P reports changed from line separated entries to tables during 2018 - the skeleton and issue orders remain unchanged</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>I analyzed the low-hanging-fruit (2019-2024) and what I found is very intriguing.</li>
    </ul>
  </li>
</ul>

    

    <ul>
      <li>Successfully extracted tables from 2019Q1 to 2025Q1</li>
  
        </ul>
      </li>
    </ul>

    <p>Initial analysis (2019–2024) yielded some interesting results:</p>
    
    <img src="images/annual-runtime-bycategory.png" alt="Annual Runtime by Category">
    <ul>
      <li>Runtime is constrained by program length</li>
      <li>Notable trends:
        <ul>
          <li>Drop in National/International coverage post-2021. Could the "unpleasantness" of conflict-related stories be causing a focusing away from those thorny topics?</li>
          <li>Rise in Weather-related stories: climate link?</li>
        </ul>
      </li>
    </ul>

    <img src="images/annual-count-issues.png" alt="Annual Count of Issues">
    <ul>
      <li>Decreasing number of issues reported post-2020 complicates earlier narratives</li>
    </ul>

    <div class="note">
      <strong>Issue:</strong> Duplicate reports for 2024 Q2 from FCC<br>
      <img src="images/2024Q2.png" alt="2024 Q2">
      <img src="images/2024Q3.png" alt="2024 Q3">
    </div>



    <p><strong>Resolution and notes from meeting:</strong> Contacted Geoffrey Graves (Engineer at WBUR) on June 6, 2025. Learned the newsroom staff creates content, with oversight before submission. $10k FCC fines due to delay in submission are possible and legal advice regarding I+P forms was recently given to the station.</p>
    <div class="quote">
      “WBUR used to submit quarterly reports ‘live’ but now delay them by one quarter.”
    </div>
    <div class="note">
  
  <ul>
    <li>
      Geoff is the person who navigates the FCC API and actually submits the I+P files for WBUR. He was previously in a engineering role at WBZ (a TV station). He's deeply knowledgeable about:
      <ul>
        <li>Radio signal transmission</li>
        <li>FCC regulations and enforcement</li>
        <li>Submission pipelines and technical compliance</li>
      </ul>
    </li>
    <li>
      The newsroom that produces the shows on WBUR has its own staff responsible for populating the documents. Final approval is done by two individuals (Geoff shared their names; I may speak with them later). Geoff then submits the files.
    </li>
    <li>
      One thing I learned: According to Mr. Graves, the decrease in runtime dedicated to issue groups was likely influenced by the cancellation of another WBUR show, <em>Radio Boston</em>. He said it was "canceled in 2025" but already "toned down" starting in 2024.
    </li>
  </ul>

  <ul>
    <li>The process of crafting the I+P lists in the past was this: The newsroom would be continuously adding content right up until submission, then rush through editing, review, and approval of a 70-page document under deadline pressure.</li>
    <li>Geoff recalled a quarter when FCC servers were slow or unresponsive. The FCC's advice in those moments? "Keep trying." Engineers like Geoff were forced to repeatedly refresh—ironically worsening server load.</li>
    <li>Despite FCC faults, any late submission could result in a $10,000 fine for the station so that was the paradigm until 2024.</li>
    <li>
      A station lawyer advised WBUR that the FCC allows a <strong>reasonable delay of one quarter</strong> to ensure complete and accurate documentation.
      <ul>
        <li>This revelation is significant: The very thing that could have corrupted my analysis—overlapping reports—may be a legally permitted practice across multiple FM stations.</li>
        <li>It may be worth scanning each station’s I+P lists to see if and when this delay paradigm appears elsewhere.</li>
      </ul>
    </li>
  </ul>
</div>

</body>
</html>
